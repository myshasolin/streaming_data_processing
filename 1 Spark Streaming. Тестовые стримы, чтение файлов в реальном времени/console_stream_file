# файл такой - 10 строк из load_iris, к которому добавлен столбец Letters, в котором перебирается одна из букв ['A', 'B', 'C', 'D', 'E']
# забросим файл на HDFS:

root@mysha-Inspiron-11-3147:/home/hsk# find / -name "iris_fifty_lines.csv"
/home/hsk/jupyter/iris_fifty_lines.csv
root@mysha-Inspiron-11-3147:/home/hsk# hdfs dfs -put /home/hsk/jupyter/iris_fifty_lines.csv /mysha/spark-stream
root@mysha-Inspiron-11-3147:/home/hsk# hdfs dfs -ls /mysha/spark-stream/
Found 2 items
-rw-r--r--   1 root supergroup        978 2023-07-28 23:21 /mysha/spark-stream/iris_fifty_lines.csv

# файл на месте, пойдём в PySpark и прочитаем его в стриме:

root@mysha-Inspiron-11-3147:/home/hsk# pyspark
Python 3.10.6 (main, May 29 2023, 11:10:38) [GCC 11.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
23/07/29 01:03:02 WARN Utils: Your hostname, mysha-Inspiron-11-3147 resolves to a loopback address: 127.0.1.1; using 192.168.1.75 instead (on interface wlp1s0)
23/07/29 01:03:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
23/07/29 01:03:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/07/29 01:03:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.3.1
      /_/

Using Python version 3.10.6 (main, May 29 2023 11:10:38)
Spark context Web UI available at http://192.168.1.75:4041
Spark context available as 'sc' (master = local[*], app id = local-1690592590026).
SparkSession available as 'spark'.
>>> 
>>> from pyspark.sql.types import StructType, StructField, StringType, DoubleType
>>> 
>>> def killAll():
...     for active_stream in spark.streams.active:
...         print("Stopping %s by killAll" % active_stream)
...         active_stream.stop()
... 
>>> def console_output(df, freq):
...     return df.writeStream \
...         .format("console") \
...         .trigger(processingTime=f"{freq} seconds") \
...         .options(truncate=False, numRows=10) \
...         .start() \
...         .awaitTermination() 
... 
>>> my_schema = StructType([
...     StructField("Letters", StringType()),
...     StructField("sepal length (cm)", DoubleType()),
...     StructField("sepal width (cm)", DoubleType()),
...     StructField("petal length (cm)", DoubleType()),
...     StructField("petal width (cm)", DoubleType())
... ])
>>> raw_files = spark \
...     .readStream \
...     .format("csv") \
...     .schema(my_schema) \
...     .options(path="/mysha/spark-stream/", header=True) \
...     .load()
>>> 
>>> out = console_output(raw_files, 5)
23/07/29 01:05:37 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-bb27b83d-b503-4a92-8a10-3c1db999a9c1. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
23/07/29 01:05:37 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
-------------------------------------------                                                                                      
Batch: 0
-------------------------------------------
+-------+-----------------+----------------+-----------------+----------------+
|Letters|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|
+-------+-----------------+----------------+-----------------+----------------+
|A      |5.1              |3.5             |1.4              |0.2             |
|B      |4.9              |3.0             |1.4              |0.2             |
|C      |4.7              |3.2             |1.3              |0.2             |
|D      |4.6              |3.1             |1.5              |0.2             |
|E      |5.0              |3.6             |1.4              |0.2             |
|A      |5.4              |3.9             |1.7              |0.4             |
|B      |4.6              |3.4             |1.4              |0.3             |
|C      |5.0              |3.4             |1.5              |0.2             |
|D      |4.4              |2.9             |1.4              |0.2             |
|E      |4.9              |3.1             |1.5              |0.1             |
+-------+-----------------+----------------+-----------------+----------------+

23/07/29 01:05:46 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 7849 milliseconds

# остановимся с помощью ctrl+C и убьём стрим:

>>> killAll()
Stopping <pyspark.sql.streaming.StreamingQuery object at 0x7fc041040700> by killAll

# уйдём из PySpark и удалим файл с HDFS:

>>> exit()
root@mysha-Inspiron-11-3147:/home/hsk# hdfs dfs -rm -skipTrash /mysha/spark-stream/iris_fifty_lines.csv
Deleted /mysha/spark-stream/iris_fifty_lines.csv



