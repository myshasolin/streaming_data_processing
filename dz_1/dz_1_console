# подключаемся:

root@mysha-Inspiron-11-3147:/home/hsk# pyspark
Python 3.10.6 (main, May 29 2023, 11:10:38) [GCC 11.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
23/07/16 20:10:53 WARN Utils: Your hostname, mysha-Inspiron-11-3147 resolves to a loopback address: 127.0.1.1; using 192.168.1.75 instead (on interface wlp1s0)
23/07/16 20:10:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
23/07/16 20:11:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.3.1
      /_/

Using Python version 3.10.6 (main, May 29 2023 11:10:38)
Spark context Web UI available at http://192.168.1.75:4040
Spark context available as 'sc' (master = local[*], app id = local-1689538264536).
SparkSession available as 'spark'.

# подгружаем библиотеки:

>>> from pyspark.sql import SparkSession, DataFrame
>>> from pyspark.sql import functions as F
>>> from pyspark.sql.types import StructType, StringType

# создаём Spark Structured Streaming-приложение на 5 строк в секунду, посмотрим сразу схему:

>>> raw_rate = spark \
...     .readStream \
...     .format("rate") \
...     .option("rowsPerSecond", 5) \
...     .load()
>>> raw_rate.printSchema()
root
 |-- timestamp: timestamp (nullable = true)
 |-- value: long (nullable = true)
 
 # метод isStreaming возвращает флаг, проверяющий, будет ли наше приложение возвращать потоковый DataFrame:
 
 >>> raw_rate.isStreaming
True

# метод writeStream записывает данные в какой-то источник, передадим ему в качестве источника окно терминала, 
# сгенерируем несколько батчей и остановимся при помощи самописной функции killAll():

>>> def killAll():
...     for active_stream in spark.streams.active:
...         print("Stopping %s by killAll" % active_stream)
...         active_stream.stop()
... 
>>> rate_stream = raw_rate \
...     .writeStream \
...     .format("console") \
...     .start()
23/07/17 01:31:40 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-5891f5e2-b068-4978-9cc2-9f4fb628ab31. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
23/07/17 01:31:40 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
>>> -------------------------------------------
Batch: 0
-------------------------------------------
+---------+-----+
|timestamp|value|
+---------+-----+
+---------+-----+

-------------------------------------------
Batch: 1
-------------------------------------------
+--------------------+-----+
|           timestamp|value|
+--------------------+-----+
|2023-07-17 01:31:...|    0|
|2023-07-17 01:31:...|    4|
|2023-07-17 01:31:...|    1|
|2023-07-17 01:31:...|    2|
|2023-07-17 01:31:...|    3|
+--------------------+-----+

-------------------------------------------
Batch: 2
-------------------------------------------
+--------------------+-----+
|           timestamp|value|
+--------------------+-----+
|2023-07-17 01:31:...|    5|
|2023-07-17 01:31:...|    9|
|2023-07-17 01:31:...|    6|
|2023-07-17 01:31:...|    7|
|2023-07-17 01:31:...|    8|
+--------------------+-----+

-------------------------------------------
Batch: 3
-------------------------------------------
+--------------------+-----+
|           timestamp|value|
+--------------------+-----+
|2023-07-17 01:31:...|   10|
|2023-07-17 01:31:...|   14|
|2023-07-17 01:31:...|   11|
|2023-07-17 01:31:...|   12|
|2023-07-17 01:31:...|   13|
+--------------------+-----+

-------------------------------------------
Batch: 4
-------------------------------------------
+--------------------+-----+
|           timestamp|value|
+--------------------+-----+
|2023-07-17 01:31:...|   15|
|2023-07-17 01:31:...|   19|
|2023-07-17 01:31:...|   16|
|2023-07-17 01:31:...|   17|
|2023-07-17 01:31:...|   18|
+--------------------+-----+

-------------------------------------------
Batch: 5
-------------------------------------------
+--------------------+-----+
|           timestamp|value|
+--------------------+-----+
|2023-07-17 01:31:...|   20|
|2023-07-17 01:31:...|   24|
|2023-07-17 01:31:...|   21|
|2023-07-17 01:31:...|   22|
|2023-07-17 01:31:...|   23|
+--------------------+-----+

-------------------------------------------
Batch: 6
-------------------------------------------
+--------------------+-----+
|           timestamp|value|
+--------------------+-----+
|2023-07-17 01:31:...|   25|
|2023-07-17 01:31:...|   29|
|2023-07-17 01:31:...|   26|
|2023-07-17 01:31:...|   27|
|2023-07-17 01:31:...|   28|
+--------------------+-----+

-------------------------------------------
Batch: 7
-------------------------------------------
+--------------------+-----+
|           timestamp|value|
+--------------------+-----+
|2023-07-17 01:31:...|   30|
|2023-07-17 01:31:...|   34|
|2023-07-17 01:31:...|   31|
|2023-07-17 01:31:...|   32|
|2023-07-17 01:31:...|   33|
+--------------------+-----+

killAll()

# первый батч пустой, а следующие 2-3 могут быть размера больше заданного, так происходит из-за того, что изначально 
# скорость генерации данных не соответствует скорости их обработки. Обычно через несколько батчей ситуация с их размерами 
# выравнивается, но вообще важно давать на их генерацию достаточно времени, что теперь и сделаем, сделаем генерацию 
# раз в 10 секунд, а в сами батчи будем добавлять только чётные строки и отмечать каждое кратное 5-и значение в столбце value
# записью "FIVE!" в овом столбце:

>>> filtered_rate = raw_rate \
...     .filter( F.col("value") % F.lit("2") == 0 )
>>> extra_rate = filtered_rate \
...     .withColumn("my_value",
...                 F.when((F.col("value") % F.lit(5) == 0), F.lit("FIVE!"))
...                     .otherwise(F.lit("")))
>>> def console_output(df, freq):
...     return df.writeStream \
...         .format("console") \
...         .trigger(processingTime='%s seconds' % freq ) \
...         .options(truncate=False) \
...         .start()
... 
>>> out = console_output(extra_rate, 10)
23/07/17 01:47:54 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-4b88422c-6eb0-4150-8df5-ba8d370103d5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
23/07/17 01:47:54 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
>>> -------------------------------------------
Batch: 0
-------------------------------------------
+---------+-----+--------+
|timestamp|value|my_value|
+---------+-----+--------+
+---------+-----+--------+

-------------------------------------------
Batch: 1
-------------------------------------------
+-----------------------+-----+--------+
|timestamp              |value|my_value|
+-----------------------+-----+--------+
|2023-07-17 01:47:54.982|0    |FIVE!   |
|2023-07-17 01:47:55.782|4    |        |
|2023-07-17 01:47:56.582|8    |        |
|2023-07-17 01:47:57.382|12   |        |
|2023-07-17 01:47:58.182|16   |        |
|2023-07-17 01:47:58.982|20   |FIVE!   |
|2023-07-17 01:47:59.782|24   |        |
|2023-07-17 01:47:55.382|2    |        |
|2023-07-17 01:47:56.182|6    |        |
|2023-07-17 01:47:56.982|10   |FIVE!   |
|2023-07-17 01:47:57.782|14   |        |
|2023-07-17 01:47:58.582|18   |        |
|2023-07-17 01:47:59.382|22   |        |
+-----------------------+-----+--------+

-------------------------------------------
Batch: 2
-------------------------------------------
+-----------------------+-----+--------+
|timestamp              |value|my_value|
+-----------------------+-----+--------+
|2023-07-17 01:48:00.182|26   |        |
|2023-07-17 01:48:00.982|30   |FIVE!   |
|2023-07-17 01:48:01.782|34   |        |
|2023-07-17 01:48:02.582|38   |        |
|2023-07-17 01:48:03.382|42   |        |
|2023-07-17 01:48:04.182|46   |        |
|2023-07-17 01:48:04.982|50   |FIVE!   |
|2023-07-17 01:48:05.782|54   |        |
|2023-07-17 01:48:06.582|58   |        |
|2023-07-17 01:48:07.382|62   |        |
|2023-07-17 01:48:08.182|66   |        |
|2023-07-17 01:48:08.982|70   |FIVE!   |
|2023-07-17 01:48:09.782|74   |        |
|2023-07-17 01:48:00.582|28   |        |
|2023-07-17 01:48:01.382|32   |        |
|2023-07-17 01:48:02.182|36   |        |
|2023-07-17 01:48:02.982|40   |FIVE!   |
|2023-07-17 01:48:03.782|44   |        |
|2023-07-17 01:48:04.582|48   |        |
|2023-07-17 01:48:05.382|52   |        |
+-----------------------+-----+--------+
only showing top 20 rows

-------------------------------------------
Batch: 3
-------------------------------------------
+-----------------------+-----+--------+
|timestamp              |value|my_value|
+-----------------------+-----+--------+
|2023-07-17 01:48:10.182|76   |        |
|2023-07-17 01:48:10.982|80   |FIVE!   |
|2023-07-17 01:48:11.782|84   |        |
|2023-07-17 01:48:12.582|88   |        |
|2023-07-17 01:48:13.382|92   |        |
|2023-07-17 01:48:14.182|96   |        |
|2023-07-17 01:48:14.982|100  |FIVE!   |
|2023-07-17 01:48:15.782|104  |        |
|2023-07-17 01:48:16.582|108  |        |
|2023-07-17 01:48:17.382|112  |        |
|2023-07-17 01:48:18.182|116  |        |
|2023-07-17 01:48:18.982|120  |FIVE!   |
|2023-07-17 01:48:19.782|124  |        |
|2023-07-17 01:48:10.582|78   |        |
|2023-07-17 01:48:11.382|82   |        |
|2023-07-17 01:48:12.182|86   |        |
|2023-07-17 01:48:12.982|90   |FIVE!   |
|2023-07-17 01:48:13.782|94   |        |
|2023-07-17 01:48:14.582|98   |        |
|2023-07-17 01:48:15.382|102  |        |
+-----------------------+-----+--------+
only showing top 20 rows

killAll()
Stopping <pyspark.sql.streaming.StreamingQuery object at 0x7feb7f0e46a0> by killAll

# всё получилось, радуемся и закрываем Spark-сессию:

>>> exit()
root@mysha-Inspiron-11-3147:/home/hsk# 


